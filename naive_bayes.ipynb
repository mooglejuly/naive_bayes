{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Experiment\n",
    "\n",
    "We implement the naive bayes algorithm from the page 7 of https://web.stanford.edu/~jurafsky/slp3/6.pdf.\n",
    "We apply the algorithm on a set of movie reviews that are already labeled as positive or negative. The data set of 1000 postive and 1000 negative samples are found here: http://www.cs.cornell.edu/people/pabo/movie-review-data/. We use polarity_dataset v2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_documents = []\n",
    "negative_documents = []\n",
    "\n",
    "import glob   \n",
    "path = 'movie_reviews_data/pos/*'   \n",
    "files=glob.glob(path)   \n",
    "for el in files:     \n",
    "    f=open(el, 'r')  \n",
    "    temp = f.readlines()   \n",
    "    positive_documents.append('. '.join(temp))\n",
    "    f.close() \n",
    "    \n",
    "path = 'movie_reviews_data/neg/*'   \n",
    "files=glob.glob(path)   \n",
    "for el in files:     \n",
    "    f=open(el, 'r')  \n",
    "    temp = f.readlines()   \n",
    "    negative_documents.append('. '.join(temp))\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(positive_documents)\n",
    "shuffle(negative_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "800\n",
      "800\n",
      "800\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "train_to_test_ratio = 4\n",
    "num_train_positive = int(len(positive_documents) * train_to_test_ratio / float(train_to_test_ratio + 1.0))\n",
    "num_train_negative = int(len(negative_documents) * train_to_test_ratio / float(train_to_test_ratio + 1.0))\n",
    "\n",
    "print(num_train_positive)\n",
    "print(num_train_negative)\n",
    "\n",
    "positive_documents_train = positive_documents[:num_train_positive]\n",
    "negative_documents_train = negative_documents[:num_train_negative]\n",
    "\n",
    "positive_documents_test = positive_documents[num_train_positive:]\n",
    "negative_documents_test = negative_documents[num_train_negative:]\n",
    "\n",
    "print(len(positive_documents_train))\n",
    "print(len(negative_documents_train))\n",
    "print(len(positive_documents_test))\n",
    "print(len(negative_documents_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('.')\n",
    "\n",
    "def get_tokens_from_document(document):\n",
    "    tokens = nltk.word_tokenize(document)\n",
    "    tokens = [x for x in tokens if x not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, positive_documents_train, negative_documents_train):\n",
    "        self.positive_documents_train_ = positive_documents_train\n",
    "        self.negative_documents_train_ = negative_documents_train\n",
    "        self.positive_label_ = \"POSITIVE\"\n",
    "        self.negative_label_ = \"NEGATIVE\"\n",
    "        self.stop_words_ = set(stopwords.words('english'))\n",
    "        self.stop_words_.add('.')\n",
    "        \n",
    "    def initialize_vocab(self, max_vocab_size):\n",
    "        token_appearance_counter = collections.defaultdict(int)\n",
    "        positive_appearance_counter = collections.defaultdict(int)\n",
    "        negative_appearance_counter = collections.defaultdict(int)\n",
    "        \n",
    "        for positive_doc in self.positive_documents_train_:\n",
    "            tokens = get_tokens_from_document(positive_doc)\n",
    "            for t in tokens:\n",
    "                token_appearance_counter[t] += 1\n",
    "                positive_appearance_counter[t] += 1\n",
    "                \n",
    "        for negative_doc in self.negative_documents_train_:\n",
    "            tokens = get_tokens_from_document(negative_doc)\n",
    "            for t in tokens:\n",
    "                token_appearance_counter[t] += 1\n",
    "                negative_appearance_counter[t] += 1\n",
    "                \n",
    "        sorted_list = sorted(token_appearance_counter.items(), key=lambda x: x[1])\n",
    "        \n",
    "        self.vocab_ = [tup[0] for tup in sorted_list]\n",
    "        if (len(self.vocab_) > max_vocab_size):\n",
    "            self.vocab_ = self.vocab_[:max_vocab_size]\n",
    "            \n",
    "        self.positive_vocab_counter_ = {}\n",
    "        self.negative_vocab_counter_ = {}\n",
    "        \n",
    "        for word in self.vocab_:\n",
    "            self.positive_vocab_counter_[word] = positive_appearance_counter[word] if word in positive_appearance_counter else 0\n",
    "            self.negative_vocab_counter_[word] = negative_appearance_counter[word] if word in negative_appearance_counter else 0\n",
    "        \n",
    "        self.positive_loglikelihood_ = {}\n",
    "        self.negative_loglikelihood_ = {}\n",
    "        \n",
    "        temp_denom = sum(self.positive_vocab_counter_.values()) + len(self.positive_vocab_counter_)\n",
    "        for word, val in self.positive_vocab_counter_.items():\n",
    "            self.positive_loglikelihood_[word] = math.log((val + 1) / float(temp_denom))\n",
    "            \n",
    "        temp_denom = sum(self.negative_vocab_counter_.values()) + len(self.negative_vocab_counter_)\n",
    "        for word, val in self.negative_vocab_counter_.items():\n",
    "            self.negative_loglikelihood_[word] = math.log((val + 1) / float(temp_denom))\n",
    "\n",
    "    def initialize(self, max_vocab_size):\n",
    "        self.log_priors_ = {}\n",
    "        num_total_documents = len(self.positive_documents_train_) + len(self.negative_documents_train_)\n",
    "        self.log_priors_[self.positive_label_] = len(self.positive_documents_train_) / float(num_total_documents)\n",
    "        self.log_priors_[self.negative_label_] = len(self.negative_documents_train_) / float(num_total_documents)\n",
    "        \n",
    "        self.initialize_vocab(max_vocab_size)\n",
    "        \n",
    "    def classify(self, document):\n",
    "        tokens = get_tokens_from_document(document)\n",
    "        scores = {}\n",
    "        scores[self.positive_label_] = self.log_priors_[self.positive_label_]\n",
    "        scores[self.negative_label_] = self.log_priors_[self.negative_label_]\n",
    "        \n",
    "        for word in tokens:\n",
    "            if word in self.positive_loglikelihood_:\n",
    "                scores[self.positive_label_] += self.positive_loglikelihood_[word]\n",
    "            scores[self.negative_label_] += self.negative_loglikelihood_[word] if word in self.negative_loglikelihood_ else 0.0\n",
    "        \n",
    "        return 1 if scores[self.positive_label_] >= scores[self.negative_label_] else -1\n",
    "        \n",
    "\n",
    "nbc = NaiveBayesClassifier(positive_documents_train, negative_documents_train)\n",
    "nbc.initialize(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "for x in positive_documents_test:\n",
    "    t += 1 if nbc.classify(x) == 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
